{
    "cells": [
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "\n\nIn case you want to learn how ETL is done, please run the following notebook first and update the file name below accordingly\n\nhttps://github.com/IBM/coursera/blob/master/coursera_ml/a2_w1_s3_ETL.ipynb\n"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# delete files from previous runs\n!rm -f hmp.parquet*\n\n# download the file containing the data in PARQUET format\n!wget https://github.com/IBM/coursera/raw/master/hmp.parquet\n    \n# create a dataframe out of it\ndf = spark.read.parquet('hmp.parquet')\n\n# register a corresponding query table\ndf.createOrReplaceTempView('df')",
            "execution_count": 1,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20191228021521-0001\nKERNEL_ID = 8186fd41-1319-4e21-9ddc-8f09de04f281\n--2019-12-28 02:15:24--  https://github.com/IBM/coursera/raw/master/hmp.parquet\nResolving github.com (github.com)... 140.82.114.4\nConnecting to github.com (github.com)|140.82.114.4|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/IBM/coursera/master/hmp.parquet [following]\n--2019-12-28 02:15:25--  https://raw.githubusercontent.com/IBM/coursera/master/hmp.parquet\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.8.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.8.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 932997 (911K) [application/octet-stream]\nSaving to: 'hmp.parquet'\n\n100%[======================================>] 932,997     --.-K/s   in 0.04s   \n\n2019-12-28 02:15:25 (24.4 MB/s) - 'hmp.parquet' saved [932997/932997]\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "splits = df.randomSplit([0.8, 0.2])\ndf_train = splits[0]\ndf_test = splits[1]",
            "execution_count": 2,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df_train.show()",
            "execution_count": 3,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "+---+---+---+--------------------+-------------+\n|  x|  y|  z|              source|        class|\n+---+---+---+--------------------+-------------+\n|  0| 10| 28|Accelerometer-201...|    Getup_bed|\n|  0| 12| 39|Accelerometer-201...|Sitdown_chair|\n|  0| 15| 39|Accelerometer-201...|  Brush_teeth|\n|  0| 16| 31|Accelerometer-201...|    Getup_bed|\n|  0| 24| 35|Accelerometer-201...|Sitdown_chair|\n|  0| 25| 30|Accelerometer-201...|  Brush_teeth|\n|  0| 25| 40|Accelerometer-201...|  Brush_teeth|\n|  0| 26| 15|Accelerometer-201...| Climb_stairs|\n|  0| 26| 42|Accelerometer-201...|  Brush_teeth|\n|  0| 27| 31|Accelerometer-201...|Sitdown_chair|\n|  0| 27| 33|Accelerometer-201...|    Getup_bed|\n|  0| 27| 37|Accelerometer-201...|  Brush_teeth|\n|  0| 27| 39|Accelerometer-201...|  Brush_teeth|\n|  0| 27| 41|Accelerometer-201...|  Brush_teeth|\n|  0| 28| 28|Accelerometer-201...|  Brush_teeth|\n|  0| 29| 17|Accelerometer-201...|    Getup_bed|\n|  0| 29| 25|Accelerometer-201...|    Getup_bed|\n|  0| 29| 25|Accelerometer-201...| Climb_stairs|\n|  0| 29| 34|Accelerometer-201...|         Walk|\n|  0| 29| 37|Accelerometer-201...|  Brush_teeth|\n+---+---+---+--------------------+-------------+\nonly showing top 20 rows\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {
                "pixiedust": {
                    "displayParams": {}
                }
            },
            "cell_type": "code",
            "source": "from pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import OneHotEncoder\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer\n\nindexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\nencoder = OneHotEncoder(inputCol=\"label\", outputCol=\"labelVec\")\nvectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],\n                                  outputCol=\"features\")\nnormalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)\n\n",
            "execution_count": 4,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from pyspark.ml.classification import LinearSVC\n\nlsvc = LinearSVC(maxIter=10, regParam=0.1)",
            "execution_count": 5,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#Query to discover distinct class values \ndf.createOrReplaceTempView('df')\ntest = spark.sql(\"SELECT DISTINCT class from df\")\ntest.show()",
            "execution_count": 6,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "+--------------+\n|         class|\n+--------------+\n| Use_telephone|\n| Standup_chair|\n|      Eat_meat|\n|     Getup_bed|\n|   Drink_glass|\n|    Pour_water|\n|     Comb_hair|\n|          Walk|\n|  Climb_stairs|\n| Sitdown_chair|\n|   Liedown_bed|\n|Descend_stairs|\n|   Brush_teeth|\n|      Eat_soup|\n+--------------+\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df.createOrReplaceTempView('df')\ndf_two_class = spark.sql(\"select * from df where class in ('Use_telephone','Standup_chair')\")\n",
            "execution_count": 7,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "splits = df_two_class.randomSplit([0.8, 0.2])\ndf_train = splits[0]\ndf_test = splits[1]",
            "execution_count": 8,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from pyspark.ml import Pipeline\npipeline = Pipeline(stages=[indexer, encoder, vectorAssembler, normalizer,lsvc])\n",
            "execution_count": 9,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "model = pipeline.fit(df_train)\n\n",
            "execution_count": 10,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "prediction = model.transform(df_train)",
            "execution_count": 11,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nevaluator.evaluate(prediction)",
            "execution_count": 12,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 12,
                    "data": {
                        "text/plain": "0.9395406906222871"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "prediction = model.transform(df_test)",
            "execution_count": 13,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nevaluator.evaluate(prediction)",
            "execution_count": 14,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 14,
                    "data": {
                        "text/plain": "0.9359381731178495"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python36",
            "display_name": "Python 3.6 with Spark",
            "language": "python3"
        },
        "language_info": {
            "mimetype": "text/x-python",
            "nbconvert_exporter": "python",
            "name": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8",
            "file_extension": ".py",
            "codemirror_mode": {
                "version": 3,
                "name": "ipython"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
