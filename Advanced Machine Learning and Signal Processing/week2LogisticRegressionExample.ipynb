{
    "cells": [
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "\n\nIn case you want to learn how ETL is done, please run the following notebook first and update the file name below accordingly\n\nhttps://github.com/IBM/coursera/blob/master/coursera_ml/a2_w1_s3_ETL.ipynb\n"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# delete files from previous runs\n!rm -f hmp.parquet*\n\n# download the file containing the data in PARQUET format\n!wget https://github.com/IBM/coursera/raw/master/hmp.parquet\n    \n# create a dataframe out of it\ndf = spark.read.parquet('hmp.parquet')\n\n# register a corresponding query table\ndf.createOrReplaceTempView('df')",
            "execution_count": 2,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "--2019-12-27 22:34:17--  https://github.com/IBM/coursera/raw/master/hmp.parquet\nResolving github.com (github.com)... 140.82.118.3\nConnecting to github.com (github.com)|140.82.118.3|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/IBM/coursera/master/hmp.parquet [following]\n--2019-12-27 22:34:18--  https://raw.githubusercontent.com/IBM/coursera/master/hmp.parquet\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 932997 (911K) [application/octet-stream]\nSaving to: 'hmp.parquet'\n\n100%[======================================>] 932,997     --.-K/s   in 0.04s   \n\n2019-12-27 22:34:18 (21.5 MB/s) - 'hmp.parquet' saved [932997/932997]\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "splits = df.randomSplit([0.8, 0.2])\ndf_train = splits[0]\ndf_test = splits[1]",
            "execution_count": 3,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Evaluating the pipeline steps\ndf_test.show()",
            "execution_count": 4,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "+---+---+---+--------------------+--------------+\n|  x|  y|  z|              source|         class|\n+---+---+---+--------------------+--------------+\n|  0| 10| 28|Accelerometer-201...|     Getup_bed|\n|  0| 11| 38|Accelerometer-201...| Sitdown_chair|\n|  0| 23| 36|Accelerometer-201...|   Brush_teeth|\n|  0| 24| 35|Accelerometer-201...| Sitdown_chair|\n|  0| 26| 15|Accelerometer-201...|  Climb_stairs|\n|  0| 29| 38|Accelerometer-201...|   Brush_teeth|\n|  0| 30| 34|Accelerometer-201...|     Getup_bed|\n|  0| 30| 38|Accelerometer-201...|  Climb_stairs|\n|  0| 31| 17|Accelerometer-201...| Standup_chair|\n|  0| 31| 28|Accelerometer-201...|  Climb_stairs|\n|  0| 31| 32|Accelerometer-201...| Standup_chair|\n|  0| 31| 35|Accelerometer-201...|   Brush_teeth|\n|  0| 32| 33|Accelerometer-201...|  Climb_stairs|\n|  0| 32| 41|Accelerometer-201...|   Brush_teeth|\n|  0| 33| 31|Accelerometer-201...|  Climb_stairs|\n|  0| 33| 38|Accelerometer-201...|Descend_stairs|\n|  0| 34| 31|Accelerometer-201...|   Brush_teeth|\n|  0| 34| 34|Accelerometer-201...|Descend_stairs|\n|  0| 34| 34|Accelerometer-201...|          Walk|\n|  0| 34| 34|Accelerometer-201...|          Walk|\n+---+---+---+--------------------+--------------+\nonly showing top 20 rows\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from pyspark.ml.feature import StringIndexer, OneHotEncoder\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer\n\n\nindexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n\nvectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],\n                                  outputCol=\"features\")\n\nnormalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=1.0)\n\n\n \n\n",
            "execution_count": 5,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Evaluating the pipeline steps\n# 1 - Indexing the class columm\nindexed = indexer.fit(df_train).transform(df_train)\nindexed.show()",
            "execution_count": 9,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "+---+---+---+--------------------+--------------+-----+\n|  x|  y|  z|              source|         class|label|\n+---+---+---+--------------------+--------------+-----+\n|  0| 12| 39|Accelerometer-201...| Sitdown_chair|  8.0|\n|  0| 15| 39|Accelerometer-201...|   Brush_teeth|  6.0|\n|  0| 16| 31|Accelerometer-201...|     Getup_bed|  1.0|\n|  0| 17| 36|Accelerometer-201...|   Brush_teeth|  6.0|\n|  0| 25| 30|Accelerometer-201...|   Brush_teeth|  6.0|\n|  0| 25| 40|Accelerometer-201...|   Brush_teeth|  6.0|\n|  0| 26| 42|Accelerometer-201...|   Brush_teeth|  6.0|\n|  0| 27| 31|Accelerometer-201...| Sitdown_chair|  8.0|\n|  0| 27| 33|Accelerometer-201...|     Getup_bed|  1.0|\n|  0| 27| 37|Accelerometer-201...|   Brush_teeth|  6.0|\n|  0| 27| 39|Accelerometer-201...|   Brush_teeth|  6.0|\n|  0| 27| 41|Accelerometer-201...|   Brush_teeth|  6.0|\n|  0| 28| 28|Accelerometer-201...|   Brush_teeth|  6.0|\n|  0| 28| 48|Accelerometer-201...|   Brush_teeth|  6.0|\n|  0| 29| 17|Accelerometer-201...|     Getup_bed|  1.0|\n|  0| 29| 25|Accelerometer-201...|     Getup_bed|  1.0|\n|  0| 29| 25|Accelerometer-201...|  Climb_stairs|  4.0|\n|  0| 29| 32|Accelerometer-201...|Descend_stairs| 10.0|\n|  0| 29| 34|Accelerometer-201...|          Walk|  0.0|\n|  0| 29| 37|Accelerometer-201...|   Brush_teeth|  6.0|\n+---+---+---+--------------------+--------------+-----+\nonly showing top 20 rows\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Evaluating the pipeline steps\n# 2 - VectorIndexing the class columm\nvectorized = vectorAssembler.transform(indexed)\nvectorized.show()",
            "execution_count": 16,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "+---+---+---+--------------------+--------------+-----+---------------+\n|  x|  y|  z|              source|         class|label|       features|\n+---+---+---+--------------------+--------------+-----+---------------+\n|  0| 12| 39|Accelerometer-201...| Sitdown_chair|  8.0|[0.0,12.0,39.0]|\n|  0| 15| 39|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,15.0,39.0]|\n|  0| 16| 31|Accelerometer-201...|     Getup_bed|  1.0|[0.0,16.0,31.0]|\n|  0| 17| 36|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,17.0,36.0]|\n|  0| 25| 30|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,25.0,30.0]|\n|  0| 25| 40|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,25.0,40.0]|\n|  0| 26| 42|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,26.0,42.0]|\n|  0| 27| 31|Accelerometer-201...| Sitdown_chair|  8.0|[0.0,27.0,31.0]|\n|  0| 27| 33|Accelerometer-201...|     Getup_bed|  1.0|[0.0,27.0,33.0]|\n|  0| 27| 37|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,27.0,37.0]|\n|  0| 27| 39|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,27.0,39.0]|\n|  0| 27| 41|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,27.0,41.0]|\n|  0| 28| 28|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,28.0,28.0]|\n|  0| 28| 48|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,28.0,48.0]|\n|  0| 29| 17|Accelerometer-201...|     Getup_bed|  1.0|[0.0,29.0,17.0]|\n|  0| 29| 25|Accelerometer-201...|     Getup_bed|  1.0|[0.0,29.0,25.0]|\n|  0| 29| 25|Accelerometer-201...|  Climb_stairs|  4.0|[0.0,29.0,25.0]|\n|  0| 29| 32|Accelerometer-201...|Descend_stairs| 10.0|[0.0,29.0,32.0]|\n|  0| 29| 34|Accelerometer-201...|          Walk|  0.0|[0.0,29.0,34.0]|\n|  0| 29| 37|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,29.0,37.0]|\n+---+---+---+--------------------+--------------+-----+---------------+\nonly showing top 20 rows\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Evaluating the pipeline steps\n# 3 - Normalizing the features columm\nnormalized = normalizer.transform(vectorized)\nnormalized.show()",
            "execution_count": 17,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "+---+---+---+--------------------+--------------+-----+---------------+--------------------+\n|  x|  y|  z|              source|         class|label|       features|       features_norm|\n+---+---+---+--------------------+--------------+-----+---------------+--------------------+\n|  0| 12| 39|Accelerometer-201...| Sitdown_chair|  8.0|[0.0,12.0,39.0]|[0.0,0.2352941176...|\n|  0| 15| 39|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,15.0,39.0]|[0.0,0.2777777777...|\n|  0| 16| 31|Accelerometer-201...|     Getup_bed|  1.0|[0.0,16.0,31.0]|[0.0,0.3404255319...|\n|  0| 17| 36|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,17.0,36.0]|[0.0,0.3207547169...|\n|  0| 25| 30|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,25.0,30.0]|[0.0,0.4545454545...|\n|  0| 25| 40|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,25.0,40.0]|[0.0,0.3846153846...|\n|  0| 26| 42|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,26.0,42.0]|[0.0,0.3823529411...|\n|  0| 27| 31|Accelerometer-201...| Sitdown_chair|  8.0|[0.0,27.0,31.0]|[0.0,0.4655172413...|\n|  0| 27| 33|Accelerometer-201...|     Getup_bed|  1.0|[0.0,27.0,33.0]|     [0.0,0.45,0.55]|\n|  0| 27| 37|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,27.0,37.0]|[0.0,0.421875,0.5...|\n|  0| 27| 39|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,27.0,39.0]|[0.0,0.4090909090...|\n|  0| 27| 41|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,27.0,41.0]|[0.0,0.3970588235...|\n|  0| 28| 28|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,28.0,28.0]|       [0.0,0.5,0.5]|\n|  0| 28| 48|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,28.0,48.0]|[0.0,0.3684210526...|\n|  0| 29| 17|Accelerometer-201...|     Getup_bed|  1.0|[0.0,29.0,17.0]|[0.0,0.6304347826...|\n|  0| 29| 25|Accelerometer-201...|     Getup_bed|  1.0|[0.0,29.0,25.0]|[0.0,0.5370370370...|\n|  0| 29| 25|Accelerometer-201...|  Climb_stairs|  4.0|[0.0,29.0,25.0]|[0.0,0.5370370370...|\n|  0| 29| 32|Accelerometer-201...|Descend_stairs| 10.0|[0.0,29.0,32.0]|[0.0,0.4754098360...|\n|  0| 29| 34|Accelerometer-201...|          Walk|  0.0|[0.0,29.0,34.0]|[0.0,0.4603174603...|\n|  0| 29| 37|Accelerometer-201...|   Brush_teeth|  6.0|[0.0,29.0,37.0]|[0.0,0.4393939393...|\n+---+---+---+--------------------+--------------+-----+---------------+--------------------+\nonly showing top 20 rows\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from pyspark.ml.classification import LogisticRegression\n\nlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)",
            "execution_count": 18,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "\nfrom pyspark.ml import Pipeline\npipeline = Pipeline(stages=[indexer, vectorAssembler, normalizer,lr])\n",
            "execution_count": 19,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "model = pipeline.fit(df_train)",
            "execution_count": 20,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "prediction = model.transform(df_train)",
            "execution_count": 21,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "prediction.printSchema()",
            "execution_count": 22,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "root\n |-- x: integer (nullable = true)\n |-- y: integer (nullable = true)\n |-- z: integer (nullable = true)\n |-- source: string (nullable = true)\n |-- class: string (nullable = true)\n |-- label: double (nullable = false)\n |-- features: vector (nullable = true)\n |-- features_norm: vector (nullable = true)\n |-- rawPrediction: vector (nullable = true)\n |-- probability: vector (nullable = true)\n |-- prediction: double (nullable = false)\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\nbinEval = MulticlassClassificationEvaluator().setMetricName(\"accuracy\") .setPredictionCol(\"prediction\").setLabelCol(\"label\")\n    \nbinEval.evaluate(prediction) ",
            "execution_count": 23,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 23,
                    "data": {
                        "text/plain": "0.20652146494726606"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "prediction = model.transform(df_test)",
            "execution_count": 24,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "binEval.evaluate(prediction) ",
            "execution_count": 25,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 25,
                    "data": {
                        "text/plain": "0.20692460273239877"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python36",
            "display_name": "Python 3.6 with Spark",
            "language": "python3"
        },
        "language_info": {
            "mimetype": "text/x-python",
            "nbconvert_exporter": "python",
            "name": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8",
            "file_extension": ".py",
            "codemirror_mode": {
                "version": 3,
                "name": "ipython"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}